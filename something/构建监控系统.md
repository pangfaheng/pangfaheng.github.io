# 如何构建一个完整的监控系统

*前言*

> *构建监控的方式有许多种，在不同的业务环境下，有不同的解决方案。*
>
> *当前主流的业务场景，是基于公有云服务搭建的微服务架构。*
>
> *在这种架构中，比较流行的方式是使用公有云基础监控+自建prometheus监控。*
>
> *使用公有云监控可以完成基础服务资源告警。*
>
> *使用自建prometheus完成告警数据收集和业务告警。*  

本文引导

> 本文先使用 jenkins + terraform ，搭建公有云持续交付流程，利用该流程，完成公有云监控的搭建。
>
> 接着使用 k8s + victoriametrics 搭建prometheus服务，virtoriametrics基于prometheus二次开发。
>
> virtoriametrics的缩写叫做vm，默认部署在k8s环境，利用k8s的高可用能力，几乎不会发生故障。
>
> grafana是一个监控视图，可以观察所有服务的监控数据，基于k8s环境。
>
> 最后利用夜莺完成告警推送、收敛等工作，这部分还不完善，有许多地方值得讨论。

## 持续交付公有云监控服务

### 在jenkins节点部署terraform

> 在jenkins节点上安装terraform，参考：https://developer.hashicorp.com/terraform/downloads，设置node节点，标签为agent

### 初始化环境

#### git仓库的代码结构

```text
terraform
    ├── tc-ap-shanghai
    │   ├── environments
    │   │   ├── main.tf
    │   │   └── provider.tf
    │   └── modules
    │       ├── monitoring
    │       │   ├── main.tf
    │       │   ├── provider.tf
    ├── terraform-apply
    │   └── Jenkinsfile
    └── terraform-plan
        └── Jenkinsfile
```

> 备注：
>
> - 该结构支持多个环境完全隔离，比如根据 “云厂商 + 地域”属性，设置tc-ap-shanghai和ali-cn-shanghai两个目录。
>
>     目录下包含了属性、资源块等资源，目录内的配置、资源、参数可以互相传递。
>
>     
>
> - tc-ap-shanghai和ali-cn-shanghai两个目录之间的配置、资源、参数不会发生干扰。
>
>     因此可以根据实际需求划分目录，比如测试环境、正式环境放在不同的目录，实现代码隔离。
>
> - terraform需要设置地域属性，但是监控服务不需要地域属性，因此在任何地域下配置都可以。
>
> - terraform状态文件存储在对象存储中，好处是可以保证数据一致性、数据安全性、数据兼容性。
>
>     注意不要覆盖其他项目的状态文件

#### 设置jenkinsfile

```jenkinsfile
# 两个文件，分别负责plan和apply，先执行plan，再执行apply
# terraform-plan/Jenkinsfile
pipeline {
  agent {
    label "agent"
  }
  stages {
    stage('检出') {
      steps {
        checkout([
          $class: 'GitSCM', 
          branches: [[name: '*/main']], 
          doGenerateSubmoduleConfigurations: false, 
          extensions: [[$class: 'CleanCheckout']], 
          submoduleCfg: [], 
          userRemoteConfigs: [[credentialsId: 'xxx-xxx', url: 'https://gitlab.com/project.git']]
        ])
      }
    }

    stage('terraform验证') {
      steps {
        sh '''
          default_dir=`pwd`
          cd terraform/
          dir_count=$(ls -l | grep "^d" |grep -v "terraform"| wc -l)
          if [ $dir_count -gt 0 ]; then
            for dir in `ls |grep -v "terraform"`; do
              env=${dir%*/}
              env=${env#*/}
              cd ${env}/environments
              terraform fmt -recursive -no-color
              terraform init -no-color
              terraform validate -no-color
              terraform plan -no-color
              cd ../../
            done
          else
          	terraform -version -no-color
            terraform fmt -recursive -no-color
            terraform init -no-color
            terraform validate -no-color
            terraform plan -no-color
          fi
        '''
      }
    }
  }
}

# terraform-apply/Jenkinsfile
pipeline {
  agent {
    label "agent"
  }
  stages {
    stage('检出') {
      steps {
        checkout([
          $class: 'GitSCM', 
          branches: [[name: '*/main']], 
          doGenerateSubmoduleConfigurations: false, 
          extensions: [[$class: 'CleanCheckout']], 
          submoduleCfg: [], 
          userRemoteConfigs: [[credentialsId: 'xxx-xxx', url: 'https://gitlab.com/project.git']]
        ])
      }
    }
    stage('terraform执行') {
      steps {
        sh '''
          default_dir=`pwd`
          cd terraform/
          dir_count=$(ls -l | grep "^d" |grep -v "terraform"| wc -l)
          if [ $dir_count -gt 0 ]; then
            for dir in `ls |grep -v "terraform"`; do
              env=${dir%*/}
              env=${env#*/}
              cd ${env}/environments
              terraform init -no-color
              terraform apply -auto-approve -no-color
              cd ../../
            done
          else
            terraform apply -auto-approve -no-color
          fi
        '''
      }
    }
  }
}


```

#### 设置jenkins流水线

注意：

- 设置两个流水线，apply设置：Build after other projects are built

- 配置环境变量：TENCENTCLOUD_SECRET_ID、TENCENTCLOUD_SECRET_KEY，如果有阿里云的服务，还需要配置阿里云的key

### 腾讯云监控配置

- 以下监控指标从腾讯云api中获取，命令：tccli monitor DescribeAllNamespaces --cli-unfold-argument --region ap-shanghai --SceneType ST_ALARM --MonitorTypes MT_QCE --Module monitor
- 腾讯云监控通知用户依赖访问管理中的用户或用户组，需要先提前设置，这里跳过不讲

terraform/tc-ap-shanghai/modules/environments/main.tf

```json
module "monitoring" {
  source = "../modules/monitoring"
}
```

terraform/tc-ap-shanghai/modules/environments/provider.tf

```json
terraform {
  required_providers {
    tencentcloud = {
      source  = "tencentcloudstack/tencentcloud"
      version = "1.81.8"
    }
  }
  # 这里写bucket所在的地域
  backend "cos" {
    region = "ap-shanghai"
    bucket = "bucket-name"
    prefix = "terraform/tc/ap-shanghai/state"
  }
}
# 这里写资源所在的地域
provider "tencentcloud" {
  region = "ap-shanghai"
}
```

terraform/tc-ap-shanghai/modules/monitoring/provider.tf

```json
terraform {
  required_providers {
    tencentcloud = {
      source  = "tencentcloudstack/tencentcloud"
      version = "1.81.8"
    }
  }
}
# 这里写资源所在的地域
provider "tencentcloud" {
  region = "ap-shanghai"
}
```

terraform/tc-ap-shanghai/modules/monitoring/main.tf

```json
resource "tencentcloud_monitor_alarm_notice" "project-notice" {
  name            = "告警接收组"
  notice_type     = "ALL"
  notice_language = "zh-CN"
  user_notices {
    receiver_type            = "GROUP"
    start_time               = 0
    end_time                 = 86399
    notice_way               = ["SMS", "EMAIL", "WECHAT", "CALL"]
    group_ids                = [腾讯云上获取用户组id]
    weekday                  = [1, 2, 3, 4, 5, 6, 7]
    phone_call_type          = "CIRCLE"
  }
  url_notices {
    url        = "http://webhook.com"
    start_time = 0
    end_time   = 86399
    weekday    = [1, 2, 3, 4, 5, 6, 7]
  }
}

resource "tencentcloud_monitor_alarm_policy" "project-policy-cvm" {
  enable       = 1
  monitor_type = "MT_QCE"
  namespace    = "cvm_device"
  notice_ids = [
    tencentcloud_monitor_alarm_notice.project-notice.id,
  ]
  policy_name = "服务器监控告警"
  project_id  = 项目id

  conditions {
    is_union_rule = 0
    # 基础cpu使用率
    rules {
      continue_period  = 5
      description      = "Basic CPU Usage"
      is_power_notice  = 0
      metric_name      = "BaseCpuUsage"
      notice_frequency = 10800
      operator         = "gt"
      period           = 60
      rule_type        = "STATIC"
      unit             = "%"
      value            = "70"
    }
    # 内存使用率
    rules {
      continue_period  = 5
      description      = "MemoryUtilization"
      is_power_notice  = 0
      metric_name      = "MemUsage"
      notice_frequency = 10800
      operator         = "gt"
      period           = 60
      rule_type        = "STATIC"
      unit             = "%"
      value            = "70"
    }
  policy_tag {
    key   = "proj"
    value = "项目"
  }
}
```

### 阿里云监控配置

- 以下代码根据使用资源组设置，所以需要提前将资源归类到一个资源组中，过程略

terraform/ali-cn-shanghai/environments/main.tf

```json
module "monitoring" {
  source = "../modules/monitoring"
}
```

terraform/ali-cn-shanghai/environments/provider.tf

```json
terraform {
  required_providers {
    alicloud = {
      source: "aliyun/alicloud"
      version: "1.208.0"
    }
  }
	# 这里写bucket所在的地域
  backend "oss" {
    profile = "terraform"
    bucket  = "bucket-name"
    prefix  = "terraform/ali/cn-shanghai/state"
    key     = "terraform.tfstate"
    region  = "oss-cn-shanghai-1"
  }
}
# 这里写资源所在的地域
provider "alicloud" {
  region = "cn-shanghai"
}
```

terraform/ali-cn-shanghai/modules/monitoring/provider.tf

```json
terraform {
  required_providers {
    alicloud = {
      source: "aliyun/alicloud"
      version: "1.208.0"
    }
  }
}
# 这里写资源所在的地域
provider "alicloud" {
  region = "cn-shanghai"
}
```

terraform/ali-cn-shanghai/modules/monitoring/main.tf

参考:

- https://help.aliyun.com/document_detail/335111.html?spm=a2c4g.95817.0.0.18f92d56s0cyLN
- https://help.aliyun.com/zh/cms/developer-reference/api-creategroupmetricrules?spm=a2c4g.11186623.0.i21
- https://cms.console.aliyun.com/metric-meta/acs_ecs_dashboard/ecs?spm=a2c4g.11186623.0.0.a0e53fe4ZmCQW7

```json
resource "alicloud_cms_alarm_contact" "monitor-contact" {
  alarm_contact_name  = "告警接收用户"
  describe            = "备注"
  channels_mail       = "邮箱"
  channels_sms        = "电话"
  channels_ding_web_hook = "webhook"
  lang = "zh-cn"
}
resource "alicloud_cms_alarm_contact_group" "monitor-contact-group" {
  alarm_contact_group_name = "告警接收组"
  contacts = [
    alicloud_cms_alarm_contact.monitor-contact.id,
  ]
}
resource "alicloud_cms_monitor_group" "monitor-resource-group" {
  contact_groups      = [alicloud_cms_alarm_contact_group.monitor-contact-group.id,]
  resource_group_id   = "资源组id"
}
resource "alicloud_cms_group_metric_rule" "monitor-acs_ecs_dashboard-cpu_total" {
  group_id               = alicloud_cms_monitor_group.monitor-resource-group.id
  group_metric_rule_name = "monitor-acs_ecs_dashboard-cpu_total"
  category               = "ecs"
  metric_name            = "cpu_total"
  namespace              = "acs_ecs_dashboard"
  rule_id                = "monitor-acs_ecs_dashboard-cpu_total"
  period                 = "60"
  silence_time           = 86400
  escalations {
    critical {
      comparison_operator = "GreaterThanOrEqualToThreshold"
      statistics          = "Average"
      threshold           = "95"
      times               = 5
    }
    warn {
      comparison_operator = "GreaterThanOrEqualToThreshold"
      statistics          = "Average"
      threshold           = "80"
      times               = 5
    }
  }
}
```



## 2. 基于victoriametrics自建监控服务中心

### 2.1 部署k8s环境

略

### 2.2 下载应用包

下载链接：https://github.com/VictoriaMetrics/operator/releases/

应用包名：bundle_crd.zip

解压缩后可以看到一些yaml文件

### 2.3 部署

#### 2.3.1 部署crd

```shel
kubectl apply -f crds/crd.yaml
kubectl apply -f operator/manager.yaml
kubectl apply -f operator/rbac.yaml
```

#### 2.3.2 在k8s上部署集群

将几个文件放到一个目录下，然后使用kubectl apply -f ./即可

- vmcluster.yaml
- vmservice.yaml
- vmagent.yaml

文件内容参考

- vmcluster.yaml

```yaml
---
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMCluster
metadata:
  name: vmcluster-persistent
  namespace: monitoring-system
spec:
  retentionPeriod: "30d"
  replicationFactor: 1
  vmstorage:
    podMetadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8482"
    replicaCount: 3
    storageDataPath: "/vm-data"
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: dick_for_vm
          resources:
            requests:
              storage: 200Gi
    extraArgs:
      search.maxUniqueTimeseries: "300000"
    resources:
      requests:
        cpu: "100m"
        memory: 500Mi
      limits:
        cpu: "1"
        memory: 1500Mi
  vmselect:
    podMetadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8481"
    replicaCount: 2
    cacheMountPath: "/select-cache"
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: dick_for_vm
          resources:
            requests:
              storage: 20Gi
    resources:
      requests:
        cpu: "100m"
        memory: 100Mi
      limits:
        cpu: "0.5"
        memory: "500Mi"
      requests:
        cpu: "0.1"
        memory: "200Mi"
    extraArgs:
      search.maxSamplesPerQuery: "100000000000"
  vminsert:
    podMetadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8480"
    replicaCount: 2
    extraArgs:
      maxLabelsPerTimeseries: "100"
    resources:
      requests:
        cpu: "0.1"
        memory: "100Mi"
      limits:
        cpu: "1"
        memory: "500Mi"
```

- vmservice.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: 子网id
  finalizers:
  - apps.victoriametrics.com/finalizer
  labels:
    app.kubernetes.io/component: monitoring
    app.kubernetes.io/instance: vmcluster
    app.kubernetes.io/name: vminsert-loadbalancer
  name: vminsert-loadbalancer
  namespace: monitoring-system
spec:
  ports:
  - name: http
    port: 8480
    protocol: TCP
    targetPort: 8480
  selector:
    app.kubernetes.io/component: monitoring
    app.kubernetes.io/instance: vmcluster
    app.kubernetes.io/name: vminsert
    managed-by: vm-operator
  sessionAffinity: None
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: 子网id
  finalizers:
  - apps.victoriametrics.com/finalizer
  labels:
    app.kubernetes.io/component: monitoring
    app.kubernetes.io/instance: vmcluster
    app.kubernetes.io/name: vmselect-loadbalancer
  name: vmselect-loadbalancer
  namespace: monitoring-system
spec:
  ports:
  - name: http
    port: 8481
    protocol: TCP
    targetPort: 8481
  selector:
    app.kubernetes.io/component: monitoring
    app.kubernetes.io/instance: vmcluster
    app.kubernetes.io/name: vmselect
    managed-by: vm-operator
  sessionAffinity: None
  type: LoadBalancer

```

- vmagent.yaml

```yaml
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMAgent
metadata:
  name: vmagent
  namespace: monitoring-system
spec:
  inlineScrapeConfig: |
    - job_name: vmagent
      static_configs:
        - targets: ["localhost:8429"]
    - job_name: "kubernetes-apiservers"
      kubernetes_sd_configs:
        - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
        - source_labels:
            [
              __meta_kubernetes_namespace,
              __meta_kubernetes_service_name,
              __meta_kubernetes_endpoint_port_name,
            ]
          action: keep
          regex: default;kubernetes;https
    - job_name: "kubernetes-nodes"
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/$1/proxy/metrics
    - job_name: "kubernetes-nodes-cadvisor"
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
      metric_relabel_configs:
        - action: replace
          source_labels: [pod]
          regex: '(.+)'
          target_label: pod_name
          replacement: '${1}'
        - action: replace
          source_labels: [container]
          regex: '(.+)'
          target_label: container_name
          replacement: '${1}'
        - action: replace
          target_label: name
          replacement: k8s_stub
        - action: replace
          source_labels: [id]
          regex: '^/system\.slice/(.+)\.service$'
          target_label: systemd_service_name
          replacement: '${1}'
    - job_name: "kubernetes-service-endpoints"
      kubernetes_sd_configs:
        - role: endpoints
      relabel_configs:
        - action: drop
          source_labels: [__meta_kubernetes_pod_container_init]
          regex: true
        - action: keep_if_equal
          source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_container_port_number]
        - source_labels:
            [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels:
            [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels:
            [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels:
            [
              __address__,
              __meta_kubernetes_service_annotation_prometheus_io_port,
            ]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_name
        - source_labels: [__meta_kubernetes_pod_node_name]
          action: replace
          target_label: kubernetes_node
    - job_name: "kubernetes-service-endpoints-slow"
      scrape_interval: 5m
      scrape_timeout: 30s
      kubernetes_sd_configs:
        - role: endpoints
      relabel_configs:
        - action: drop
          source_labels: [__meta_kubernetes_pod_container_init]
          regex: true
        - action: keep_if_equal
          source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_container_port_number]
        - source_labels:
            [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
          action: keep
          regex: true
        - source_labels:
            [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels:
            [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels:
            [
              __address__,
              __meta_kubernetes_service_annotation_prometheus_io_port,
            ]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_name
        - source_labels: [__meta_kubernetes_pod_node_name]
          action: replace
          target_label: kubernetes_node
    - job_name: "kubernetes-services"
      metrics_path: /probe
      params:
        module: [http_2xx]
      kubernetes_sd_configs:
        - role: service
      relabel_configs:
        - source_labels:
            [__meta_kubernetes_service_annotation_prometheus_io_probe]
          action: keep
          regex: true
        - source_labels: [__address__]
          target_label: __param_target
        - target_label: __address__
          replacement: blackbox
        - source_labels: [__param_target]
          target_label: instance
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          target_label: kubernetes_name
    - job_name: "kubernetes-pods"
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        - action: drop
          source_labels: [__meta_kubernetes_pod_container_init]
          regex: true
        - action: keep_if_equal
          source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_container_port_number]
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels:
            [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
  replicaCount: 1
  remoteWrite:
    - url: "http://vminsert-loadbalancer.monitoring-system:8480/insert/{accountid}/prometheus"
  externalLabels:
    cluster: "{cluster_name}"
  extraArgs:
    promscrape.maxScrapeSize: "167772160"
  resources:
    requests:
      cpu: "250m"
      memory: "250Mi"
    limits:
      cpu: "1000m"
      memory: "1000Mi"
```

#### 2.3.3 二进制部署vmagent

场景：多个项目时，需要在其他vpc的虚拟机上部署vmagent。因为网络不通，可以使用二进制方式部署，通过公网发送数据到vm集群。（注意k8s上的vminsert需要有公网入口）

- 先下载vmutil包：https://github.com/VictoriaMetrics/VictoriaMetrics/releases/download/v1.91.1/vmutils-linux-amd64-v1.91.1.tar.gz

- 解压后，创建配置文件，命名为prometheus.yaml，内容如下，业务配置参考prometheus文档，推荐使用cousul_sd_config

```yaml
global:
  scrape_interval: 60s
scrape_configs:
	- job_name: vmagent
		static_configs:
			- targets: ["localhost:8429"]
```

- 设置开机启动，sudo systemctl enable --now vmagent

```service
[Unit]
Description=vmagent
Documentation=https://github.com/VictoriaMetrics/VictoriaMetrics/releases
After=network.target
[Service]
Type=simple
User=vmutils
ExecStart=/vmagent-prod -promscrape.config=prometheus.yml -remoteWrite.tmpDataPath /tmp_data -remoteWrite.url=http://vminsert.com/insert/{accountid}/prometheus
Restart=on-failure
[Install]
WantedBy=multi-user.target
```

## 3. 基于夜莺部署告警服务

### 3.1 部署夜莺

参考文档：https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v6/install/intro/

```shell
# 下载
mkdir -p /opt/n9e && cd /opt/n9e
tarball=n9e-v6.0.0-ga.9.2-linux-amd64.tar.gz
urlpath=https://download.flashcat.cloud/${tarball}
wget -q $urlpath || exit 1
tar zxvf ${tarball}
# 安装redis
yum install redis
# 修改配置 etc/config.toml
DSN="n9e_v6:${mysql_passwd}@tcp(${mysql_url}:${mysql_port})/n9e_v6?charset=utf8mb4&parseTime=True&loc=Local&allowNativePasswords=true"
# 导入sql
mysql -uroot -p1234 < n9e.sql
# 启动
nohup ./n9e &> n9e.log &
ss -tlnp|grep 17000
```

### 3.2 配置资源与告警规则

- 与grafana类似，不做过多描述。
- 夜莺有内置监控，克隆即可。
- 短信在smtp中设置。
- 飞书通知模板可以参考，注意标签需要设置

```
{{- $labels := .TagsMap }}
级别状态：S{{.Severity}} {{if .IsRecovered}}恢复{{else}}触发{{end}}
规则名称：{{.RuleName}}{{if .RuleNote}}
项目名称｜所属地域：{{$labels.project}}｜{{$labels.region}}
告警对象：{{$labels.host}}
告警详情：{{.RuleNote}}{{end}}
触发时值：{{.TriggerValue}}
{{if .IsRecovered}}恢复时间：{{timeformat .LastEvalTime}}{{else}}触发时间：{{timeformat .TriggerTime}}{{end}}
发送时间：{{timestamp}}
所有监控指标：{{.TagsJSON}}
```



### 3.3 配置告警回调地址

在系统配置中，设置回调地址，将告警信息发给自己写的脚本，处理数据后调用电话、短信api，或者发给第三方自动化工具，比如发给awx，由awx自动处理进程

参考脚本

```python
#! /usr/bin/env python3
import os
import requests
import configvalues as configvalues
from flask import Flask, request

'''
--- 该脚本接受监控数据，利用监控数据中的标签，发送给awx重启相关进程 ---
--- data为监控数据，带is_recovered的是恢复数据（这种不要），tags为标签 ---
--- 需要在awx中设置模板、账户密码、清单等 ---
--- awx_url为模版路径，如https://awx.com/api/v2/job_templates/编号/launch/，在网页端的api/v2/路径下查询编号 ---
--- inventory_id为清单id，打开awx清单，检查url上的数字即可 ---
'''

def postawx(data):
    if not data['is_recovered']:
        data = data['tags']
    else:
        return 0
    headers = {
        'Content-Type': 'application/json'
    }
    json_data = {
        "credential_passwords": {},
        "inventory_id": inventory_id,
        "extra_vars": {
            "region": data['region'],
          	"project": data['project'],
            ......
        }
    }
    response = requests.post(awx_url,auth=(awx_username,awx_password),headers=headers, json=json_data)

app = Flask(__name__)
@app.route('/webhook', methods=['POST'])
def webhook():
    data = request.get_json()
    postawx(data)
 
if __name__ == '__main__':
    app.run(debug=True, port=8081, host="0.0.0.0")
```





